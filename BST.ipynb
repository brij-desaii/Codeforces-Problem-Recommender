{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5chOLwu7CEX"
      },
      "source": [
        "## Content\n",
        "\n",
        "In this blog post, we will define our transformer model and generate personalized recommendations based on user sequences at problemLens dataset. From data pre-processing and model training to making the final predictions, we will go through all steps one by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TSj6Zhto6v_y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brij/.local/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/home/brij/.local/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewsQzPX17M6i"
      },
      "source": [
        "# 1. Data Preprocessing\n",
        "In this section, we'll start by loading the problemLens dataset. We will then construct vocabularies for problem IDs and user IDs, and create sequences of user interactions. These steps lay the groundwork for our recommendation model, converting the data into a format that our model can utilize effectively.\n",
        "## 1.1 Loading Dataset\n",
        "At first we will download our dataset to generate our sequences and vocabularies. Then user_id and problem_id values are processesed to fix their data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4x3UJlOc7llA"
      },
      "outputs": [],
      "source": [
        "# urlretrieve(\"http://files.grouplens.org/datasets/problemlens/ml-1m.zip\", \"problemlens.zip\")\n",
        "# ZipFile(\"problemlens.zip\", \"r\").extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppvDGJzE7MtC",
        "outputId": "4c0ab548-640e-4b46-ab07-78c853351ba0"
      },
      "outputs": [],
      "source": [
        "interactions = pd.read_csv(\n",
        "    \"users_problems.csv\",\n",
        "    sep=\",\",\n",
        "    names=[\"user_id\", \"problem_id\", \"timestamp\"],\n",
        "    skiprows=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eJoH9z2D7L3k"
      },
      "outputs": [],
      "source": [
        "# Preventing ids to be written as integer or float data type\n",
        "\n",
        "interactions[\"problem_id\"] = interactions[\"problem_id\"].apply(lambda x: f\"problem_{x}\")\n",
        "interactions[\"user_id\"] = interactions[\"user_id\"].apply(lambda x: f\"user_{x}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "YaE-F_nGMwui",
        "outputId": "679df010-49ce-4899-be8f-13e6658f70b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>problem_id</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_orzdevinwang</td>\n",
              "      <td>problem_1428:G2</td>\n",
              "      <td>1602994360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_orzdevinwang</td>\n",
              "      <td>problem_1428:G1</td>\n",
              "      <td>1602992678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_orzdevinwang</td>\n",
              "      <td>problem_1428:F</td>\n",
              "      <td>1602985166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_orzdevinwang</td>\n",
              "      <td>problem_1428:E</td>\n",
              "      <td>1602983894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_orzdevinwang</td>\n",
              "      <td>problem_1428:D</td>\n",
              "      <td>1602983856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14108</th>\n",
              "      <td>user_sv1shan</td>\n",
              "      <td>problem_1416:C</td>\n",
              "      <td>1665427829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14109</th>\n",
              "      <td>user_sv1shan</td>\n",
              "      <td>problem_1499:D</td>\n",
              "      <td>1665339759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14110</th>\n",
              "      <td>user_sv1shan</td>\n",
              "      <td>problem_1370:E</td>\n",
              "      <td>1665305440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14111</th>\n",
              "      <td>user_sv1shan</td>\n",
              "      <td>problem_850:B</td>\n",
              "      <td>1665257679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14112</th>\n",
              "      <td>user_sv1shan</td>\n",
              "      <td>problem_1716:D</td>\n",
              "      <td>1665175364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14113 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 user_id       problem_id   timestamp\n",
              "0      user_orzdevinwang  problem_1428:G2  1602994360\n",
              "1      user_orzdevinwang  problem_1428:G1  1602992678\n",
              "2      user_orzdevinwang   problem_1428:F  1602985166\n",
              "3      user_orzdevinwang   problem_1428:E  1602983894\n",
              "4      user_orzdevinwang   problem_1428:D  1602983856\n",
              "...                  ...              ...         ...\n",
              "14108       user_sv1shan   problem_1416:C  1665427829\n",
              "14109       user_sv1shan   problem_1499:D  1665339759\n",
              "14110       user_sv1shan   problem_1370:E  1665305440\n",
              "14111       user_sv1shan    problem_850:B  1665257679\n",
              "14112       user_sv1shan   problem_1716:D  1665175364\n",
              "\n",
              "[14113 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0CZxBN8IHK"
      },
      "source": [
        "## 1.2 Creating Vocabulary\n",
        "Now that we have our data ready, it's time to prepare our vocabularies for user IDs and problem IDs. This step will convert the unique IDs into numerical indices that our model can use. The following code snippet accomplishes this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rcNNeirg8F2X"
      },
      "outputs": [],
      "source": [
        "# Genarting a list of unique problem ids\n",
        "problem_ids = interactions.problem_id.unique()\n",
        "\n",
        "# Counter is used to feed problems to movive_vocab\n",
        "problem_counter = Counter(problem_ids)\n",
        "\n",
        "# Genarting vocabulary\n",
        "problem_vocab = vocab(problem_counter, specials=['<unk>'])\n",
        "\n",
        "# For indexing input ids\n",
        "problem_vocab_stoi = problem_vocab.get_stoi()\n",
        "\n",
        "# problem to title mapping dictionary\n",
        "# problem_title_dict = dict(zip(problems.problem_id, problems.title))\n",
        "\n",
        "# Similarly generating a vocabulary for user ids\n",
        "user_ids = interactions.user_id.unique()\n",
        "user_counter = Counter(user_ids)\n",
        "user_vocab = vocab(user_counter, specials=['<unk>'])\n",
        "user_vocab_stoi = user_vocab.get_stoi()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxhd4YPy9WNX"
      },
      "source": [
        "## 1.3 Generating Sequences\n",
        "All interactions of users are first sorted by their interaction timestamp and then divided into sub sequences to train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zvt_l0sWNF_T"
      },
      "outputs": [],
      "source": [
        "# Group ratings by user_id in order of increasing unix_timestamp.\n",
        "ratings_group = interactions.sort_values(by=[\"timestamp\"]).groupby(\"user_id\")\n",
        "\n",
        "interactions_data = pd.DataFrame(\n",
        "    data={\n",
        "        \"user_id\": list(ratings_group.groups.keys()),\n",
        "        \"problem_ids\": list(ratings_group.problem_id.apply(list)),\n",
        "        \"timestamps\": list(ratings_group.timestamp.apply(list)),\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CmAcQ8fzNPRi",
        "outputId": "0d79ae68-7ca8-467a-b492-74b6d873a810"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>problem_ids</th>\n",
              "      <th>timestamps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[problem_1200:E, problem_1451:F, problem_723:A...</td>\n",
              "      <td>[1606018760, 1606019571, 1606042942, 160622532...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_0x0002</td>\n",
              "      <td>[problem_1842:G, problem_1824:B2, problem_1824...</td>\n",
              "      <td>[1705650931, 1705651653, 1705651717, 170567142...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_160cm</td>\n",
              "      <td>[problem_104789:A, problem_104789:B, problem_1...</td>\n",
              "      <td>[1700213033, 1700214053, 1700467666, 170047034...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_1L1YA</td>\n",
              "      <td>[problem_1270:G, problem_1601:D, problem_1882:...</td>\n",
              "      <td>[1695582688, 1695642605, 1695678230, 169570815...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_36champ</td>\n",
              "      <td>[problem_1718:B, problem_1722:A, problem_1722:...</td>\n",
              "      <td>[1660672497, 1662086644, 1662086765, 166208730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>user_yyyz04</td>\n",
              "      <td>[problem_1582:F2, problem_1582:G, problem_1601...</td>\n",
              "      <td>[1635239587, 1635248059, 1635305780, 163567526...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>user_zdc123456</td>\n",
              "      <td>[problem_1779:E, problem_1442:D, problem_743:C...</td>\n",
              "      <td>[1674379798, 1674388745, 1674457695, 167446782...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>user_zhouqixuan1</td>\n",
              "      <td>[problem_1928:F, problem_1928:E, problem_1928:...</td>\n",
              "      <td>[1707720100, 1707720144, 1707720204, 170772023...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>user_zjy2008</td>\n",
              "      <td>[problem_1806:D, problem_156:D, problem_1603:C...</td>\n",
              "      <td>[1679151237, 1679190604, 1679194023, 167927464...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[problem_1788:A, problem_1788:B, problem_1788:...</td>\n",
              "      <td>[1676007774, 1676007792, 1676007805, 167600781...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>116 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              user_id                                        problem_ids  \\\n",
              "0       user_0wuming0  [problem_1200:E, problem_1451:F, problem_723:A...   \n",
              "1         user_0x0002  [problem_1842:G, problem_1824:B2, problem_1824...   \n",
              "2          user_160cm  [problem_104789:A, problem_104789:B, problem_1...   \n",
              "3          user_1L1YA  [problem_1270:G, problem_1601:D, problem_1882:...   \n",
              "4        user_36champ  [problem_1718:B, problem_1722:A, problem_1722:...   \n",
              "..                ...                                                ...   \n",
              "111       user_yyyz04  [problem_1582:F2, problem_1582:G, problem_1601...   \n",
              "112    user_zdc123456  [problem_1779:E, problem_1442:D, problem_743:C...   \n",
              "113  user_zhouqixuan1  [problem_1928:F, problem_1928:E, problem_1928:...   \n",
              "114      user_zjy2008  [problem_1806:D, problem_156:D, problem_1603:C...   \n",
              "115       user_zlxFTH  [problem_1788:A, problem_1788:B, problem_1788:...   \n",
              "\n",
              "                                            timestamps  \n",
              "0    [1606018760, 1606019571, 1606042942, 160622532...  \n",
              "1    [1705650931, 1705651653, 1705651717, 170567142...  \n",
              "2    [1700213033, 1700214053, 1700467666, 170047034...  \n",
              "3    [1695582688, 1695642605, 1695678230, 169570815...  \n",
              "4    [1660672497, 1662086644, 1662086765, 166208730...  \n",
              "..                                                 ...  \n",
              "111  [1635239587, 1635248059, 1635305780, 163567526...  \n",
              "112  [1674379798, 1674388745, 1674457695, 167446782...  \n",
              "113  [1707720100, 1707720144, 1707720204, 170772023...  \n",
              "114  [1679151237, 1679190604, 1679194023, 167927464...  \n",
              "115  [1676007774, 1676007792, 1676007805, 167600781...  \n",
              "\n",
              "[116 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interactions_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D3HT8E6jNdID"
      },
      "outputs": [],
      "source": [
        "# Sequence length, min history count and window slide size\n",
        "sequence_length = 8\n",
        "min_history = 1\n",
        "step_size = 2\n",
        "\n",
        "# Creating sequences from lists with sliding window\n",
        "def create_sequences(values, window_size, step_size, min_history):\n",
        "  sequences = []\n",
        "  start_index = 0\n",
        "  while len(values[start_index:]) > min_history:\n",
        "    seq = values[start_index : start_index + window_size]\n",
        "    sequences.append(seq)\n",
        "    start_index += step_size\n",
        "  return sequences\n",
        "\n",
        "interactions_data.problem_ids = interactions_data.problem_ids.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size, min_history)\n",
        ")\n",
        "\n",
        "\n",
        "del interactions_data[\"timestamps\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XS8uSzOVNgcD",
        "outputId": "42d197b3-c9a4-4b82-b345-00332f097b54"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>problem_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[[problem_1200:E, problem_1451:F, problem_723:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_0x0002</td>\n",
              "      <td>[[problem_1842:G, problem_1824:B2, problem_182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_160cm</td>\n",
              "      <td>[[problem_104789:A, problem_104789:B, problem_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_1L1YA</td>\n",
              "      <td>[[problem_1270:G, problem_1601:D, problem_1882...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_36champ</td>\n",
              "      <td>[[problem_1718:B, problem_1722:A, problem_1722...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>user_yyyz04</td>\n",
              "      <td>[[problem_1582:F2, problem_1582:G, problem_160...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>user_zdc123456</td>\n",
              "      <td>[[problem_1779:E, problem_1442:D, problem_743:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>user_zhouqixuan1</td>\n",
              "      <td>[[problem_1928:F, problem_1928:E, problem_1928...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>user_zjy2008</td>\n",
              "      <td>[[problem_1806:D, problem_156:D, problem_1603:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[[problem_1788:A, problem_1788:B, problem_1788...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>116 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              user_id                                        problem_ids\n",
              "0       user_0wuming0  [[problem_1200:E, problem_1451:F, problem_723:...\n",
              "1         user_0x0002  [[problem_1842:G, problem_1824:B2, problem_182...\n",
              "2          user_160cm  [[problem_104789:A, problem_104789:B, problem_...\n",
              "3          user_1L1YA  [[problem_1270:G, problem_1601:D, problem_1882...\n",
              "4        user_36champ  [[problem_1718:B, problem_1722:A, problem_1722...\n",
              "..                ...                                                ...\n",
              "111       user_yyyz04  [[problem_1582:F2, problem_1582:G, problem_160...\n",
              "112    user_zdc123456  [[problem_1779:E, problem_1442:D, problem_743:...\n",
              "113  user_zhouqixuan1  [[problem_1928:F, problem_1928:E, problem_1928...\n",
              "114      user_zjy2008  [[problem_1806:D, problem_156:D, problem_1603:...\n",
              "115       user_zlxFTH  [[problem_1788:A, problem_1788:B, problem_1788...\n",
              "\n",
              "[116 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interactions_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d1OO4GqD8NX0"
      },
      "outputs": [],
      "source": [
        "# Sub-sequences are exploded.\n",
        "# Since there might be more than one sequence for each user.\n",
        "interactions_data_transformed = interactions_data[[\"user_id\", \"problem_ids\"]].explode(\n",
        "    \"problem_ids\", ignore_index=True\n",
        ")\n",
        "\n",
        "interactions_data_transformed.rename(\n",
        "    columns={\"problem_ids\": \"sequence_problem_ids\"},\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q21pt9ww9seT",
        "outputId": "6f4b468f-5b23-4e43-be3a-e087ae022f8b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>sequence_problem_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_devinqu</td>\n",
              "      <td>[problem_551:C, problem_1406:D, problem_1579:G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_heaksicn</td>\n",
              "      <td>[problem_311:E, problem_875:F, problem_1181:E1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_wxhtzdy</td>\n",
              "      <td>[problem_437:B, problem_1310:A, problem_449:B,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_AmirrzwM</td>\n",
              "      <td>[problem_597:C, problem_576:D, problem_1198:D,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_akua</td>\n",
              "      <td>[problem_1761:D, problem_1698:E, problem_1215:...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         user_id                               sequence_problem_ids\n",
              "0   user_devinqu  [problem_551:C, problem_1406:D, problem_1579:G...\n",
              "1  user_heaksicn  [problem_311:E, problem_875:F, problem_1181:E1...\n",
              "2   user_wxhtzdy  [problem_437:B, problem_1310:A, problem_449:B,...\n",
              "3  user_AmirrzwM  [problem_597:C, problem_576:D, problem_1198:D,...\n",
              "4      user_akua  [problem_1761:D, problem_1698:E, problem_1215:..."
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interactions_data_transformed.sample(frac=1).reset_index(drop=True).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0wqTDXi_1vi"
      },
      "source": [
        "## 1.4 Train Test Split\n",
        "The data is split into training and testing sets. Although considering timestamps could potentially provide a more refined split, for the sake of simplicity, we opt for a random indexing approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5WOLmSIz9tYQ"
      },
      "outputs": [],
      "source": [
        "# Random indexing\n",
        "random_selection = np.random.rand(len(interactions_data_transformed.index)) <= 0.85\n",
        "\n",
        "# Split train data\n",
        "df_train_data = interactions_data_transformed[random_selection]\n",
        "train_data_raw = df_train_data[[\"user_id\", \"sequence_problem_ids\"]].values\n",
        "\n",
        "# Split test data\n",
        "df_test_data = interactions_data_transformed[~random_selection]\n",
        "test_data_raw = df_test_data[[\"user_id\", \"sequence_problem_ids\"]].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "j3siVHtCOWSn",
        "outputId": "d776d0d5-9877-49b9-da75-93e3c6331e4a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>sequence_problem_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[problem_1200:E, problem_1451:F, problem_723:A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[problem_723:A, problem_1344:A, problem_1344:B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[problem_1344:B, problem_1344:C, problem_1364:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[problem_1364:A, problem_469:A, problem_262:A,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_0wuming0</td>\n",
              "      <td>[problem_262:A, problem_1426:A, problem_734:B,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7018</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[problem_1809:D, problem_1809:E, problem_786:B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7020</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[problem_1837:B, problem_1837:C, problem_1837:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7021</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[problem_1837:D, problem_1837:E, problem_1837:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7022</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[problem_1837:F, problem_1824:A, problem_1824:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7023</th>\n",
              "      <td>user_zlxFTH</td>\n",
              "      <td>[problem_1824:B1, problem_1824:B2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5919 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            user_id                               sequence_problem_ids\n",
              "0     user_0wuming0  [problem_1200:E, problem_1451:F, problem_723:A...\n",
              "1     user_0wuming0  [problem_723:A, problem_1344:A, problem_1344:B...\n",
              "2     user_0wuming0  [problem_1344:B, problem_1344:C, problem_1364:...\n",
              "3     user_0wuming0  [problem_1364:A, problem_469:A, problem_262:A,...\n",
              "4     user_0wuming0  [problem_262:A, problem_1426:A, problem_734:B,...\n",
              "...             ...                                                ...\n",
              "7018    user_zlxFTH  [problem_1809:D, problem_1809:E, problem_786:B...\n",
              "7020    user_zlxFTH  [problem_1837:B, problem_1837:C, problem_1837:...\n",
              "7021    user_zlxFTH  [problem_1837:D, problem_1837:E, problem_1837:...\n",
              "7022    user_zlxFTH  [problem_1837:F, problem_1824:A, problem_1824:...\n",
              "7023    user_zlxFTH                 [problem_1824:B1, problem_1824:B2]\n",
              "\n",
              "[5919 rows x 2 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVKMFIwd_6vK"
      },
      "source": [
        "DataLoader is defined to be used for training and evaluation as final pre-processing step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YAjJ0nTp_4uX"
      },
      "outputs": [],
      "source": [
        "# Pytorch Dataset for user interactions\n",
        "class problemSeqDataset(Dataset):\n",
        "    # Initialize dataset\n",
        "    def __init__(self, data, problem_vocab_stoi, user_vocab_stoi):\n",
        "        self.data = data\n",
        "\n",
        "        self.problem_vocab_stoi = problem_vocab_stoi\n",
        "        self.user_vocab_stoi = user_vocab_stoi\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # Fetch data from the dataset\n",
        "    def __getitem__(self, idx):\n",
        "        user, problem_sequence = self.data[idx]\n",
        "        # Directly index into the vocabularies\n",
        "        problem_data = [self.problem_vocab_stoi[item] for item in problem_sequence]\n",
        "        user_data = self.user_vocab_stoi[user]\n",
        "        return torch.tensor(problem_data), torch.tensor(user_data)\n",
        "\n",
        "\n",
        "# Collate function and padding\n",
        "def collate_batch(batch):\n",
        "    problem_list = [item[0] for item in batch]\n",
        "    user_list = [item[1] for item in batch]\n",
        "    return pad_sequence(problem_list, padding_value=problem_vocab_stoi['<unk>'], batch_first=True), torch.stack(user_list)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "# Create instances of your Dataset for each set\n",
        "train_dataset = problemSeqDataset(train_data_raw, problem_vocab_stoi, user_vocab_stoi)\n",
        "val_dataset = problemSeqDataset(test_data_raw, problem_vocab_stoi, user_vocab_stoi)\n",
        "# Create DataLoaders\n",
        "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=collate_batch)\n",
        "val_iter = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRSuW2NXAkfk"
      },
      "source": [
        "# 2. Model Definition\n",
        "In this section we will define and initialize our model. Then the model will be trained with our previously generated dataset.\n",
        "## 2.1 Positional Encoder\n",
        "We start by defining the positional encoder, which is crucial for sequence-based models like the Transformer. This encoder will capture the positions of problem interactions in our sequences, thus embedding the order information that the Transformer model needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YIOSMBCtAanG"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "\n",
        "        # `div_term` is used in the calculation of the sinusoidal values.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Initializing positional encoding matrix with zeros.\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "\n",
        "        # Calculating the positional encodings.\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4wtPnqQAsOU"
      },
      "source": [
        "## 2.2 Transformer Model\n",
        "Following the definition of our positional encoder, we then establish our transformer model. This model takes both the user id and the problem id sequence as input, and it is responsible for generating the output problem predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zef8tq8NAo7M"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken: int, nuser: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        # positional encoder\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Multihead attention mechanism.\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "        # Embedding layers\n",
        "        self.problem_embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.user_embedding = nn.Embedding(nuser, d_model)\n",
        "\n",
        "        # Defining the size of the input to the model.\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear layer to map the output toproblem vocabulary.\n",
        "        self.linear = nn.Linear(2*d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        # Initializing the weights of the embedding and linear layers.\n",
        "        initrange = 0.1\n",
        "        self.problem_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.user_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, user: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        # Embedding problem ids and userid\n",
        "        problem_embed = self.problem_embedding(src) * math.sqrt(self.d_model)\n",
        "        user_embed = self.user_embedding(user) * math.sqrt(self.d_model)\n",
        "\n",
        "        # positional encoding\n",
        "        problem_embed = self.pos_encoder(problem_embed)\n",
        "\n",
        "        # generating output with final layers\n",
        "        output = self.transformer_encoder(problem_embed, src_mask)\n",
        "\n",
        "        # Expand user_embed tensor along the sequence length dimension\n",
        "        user_embed = user_embed.expand(-1, output.size(1), -1)\n",
        "\n",
        "        # Concatenate user embeddings with transformer output\n",
        "        output = torch.cat((output, user_embed), dim=-1)\n",
        "\n",
        "        output = self.linear(output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M5XCLFG_LLi"
      },
      "source": [
        "Following the model definitions, we proceed to initialize our model using a set of arbitrarily selected hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pEgP1gg-re5",
        "outputId": "95c748d3-5f81-4915-f466-b977e3354498"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brij/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "ntokens = len(problem_vocab)  # size of vocabulary\n",
        "nusers = len(user_vocab)\n",
        "emsize = 128  # embedding dimension\n",
        "d_hid = 128  # dimension of the feedforward network model\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TransformerModel(ntokens, nusers, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 1.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c8xnSds_Pf-"
      },
      "source": [
        "# 3. Train & Evaluation\n",
        "We're now ready to kick off the training process with our model, where it will learn from the dataset we've prepared. Following the training phase, we'll evaluate how well our model performs on unseen data to check its effectiveness.\n",
        "## 3.1 Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gW7vxqTV_Kn-"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_iter, epoch) -> None:\n",
        "    # Switch to training mode\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (problem_data, user_data) in enumerate(train_iter):\n",
        "        # Load problem sequence and user id\n",
        "        problem_data, user_data = problem_data.to(device), user_data.to(device)\n",
        "        user_data = user_data.reshape(-1, 1)\n",
        "\n",
        "        # Split problem sequence to inputs and targets\n",
        "        inputs, targets = problem_data[:, :-1], problem_data[:, 1:]\n",
        "        targets_flat = targets.reshape(-1)\n",
        "\n",
        "        # Predict problems\n",
        "        output = model(inputs, user_data)\n",
        "        output_flat = output.reshape(-1, ntokens)\n",
        "\n",
        "        # Backpropogation process\n",
        "        loss = criterion(output_flat, targets_flat)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        # Results\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKnPqYte_6si"
      },
      "source": [
        "## 3.2 Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ex23S0dK_kM3"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    # Switch the model to evaluation mode.\n",
        "    # This is necessary for layers like dropout,\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (problem_data, user_data) in enumerate(eval_data):\n",
        "            # Load problem sequence and user id\n",
        "            problem_data, user_data = problem_data.to(device), user_data.to(device)\n",
        "            user_data = user_data.reshape(-1, 1)\n",
        "            # Split problem sequence to inputs and targets\n",
        "            inputs, targets = problem_data[:, :-1], problem_data[:, 1:]\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            # Predict problems\n",
        "            output = model(inputs, user_data)\n",
        "            output_flat = output.reshape(-1, ntokens)\n",
        "            # Calculate loss\n",
        "            loss = criterion(output_flat, targets_flat)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJNdsU06_2ky"
      },
      "source": [
        "## 3.3 Train & Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGs7vxT0_18W",
        "outputId": "ceda42cc-c72c-4c5f-fc25-a4e1e7d17a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.60s | valid loss 10.55 | valid ppl 38127.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.33s | valid loss 10.25 | valid ppl 28241.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.34s | valid loss  9.97 | valid ppl 21368.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  0.34s | valid loss  9.63 | valid ppl 15264.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  0.34s | valid loss  9.24 | valid ppl 10322.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  0.33s | valid loss  8.80 | valid ppl  6665.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  0.34s | valid loss  8.34 | valid ppl  4184.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  0.34s | valid loss  7.88 | valid ppl  2654.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  0.34s | valid loss  7.43 | valid ppl  1690.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  0.34s | valid loss  7.00 | valid ppl  1091.76\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 10\n",
        "\n",
        "with TemporaryDirectory() as tempdir:\n",
        "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        train(model, train_iter, epoch)\n",
        "\n",
        "        # Evaluation\n",
        "        val_loss = evaluate(model, val_iter)\n",
        "\n",
        "        # Compute the perplexity of the validation loss\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "\n",
        "        # Results\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDZJozuLjIUw"
      },
      "source": [
        "## 3.4 Generating Popular problem Recommendations as Baseline\n",
        "In order to compare our model success a baseline recommendation method is required. One of the easiest recommendation method is popular problem recommendation which is obtained by most frequent and highly rated problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_buSM6b5YHuK"
      },
      "outputs": [],
      "source": [
        "# def get_popular_problems(interactions):\n",
        "#   # Calculate the number of ratings for each problem\n",
        "#   rating_counts = interactions['problem_id'].value_counts().reset_index()\n",
        "#   rating_counts.columns = ['problem_id', 'rating_count']\n",
        "\n",
        "#   # Get the most frequently rated problems\n",
        "#   min_ratings_threshold = rating_counts['rating_count'].quantile(0.95)\n",
        "\n",
        "#   # Filter problems based on the minimum number of ratings\n",
        "#   popular_problems = interactions.merge(rating_counts, on='problem_id')\n",
        "#   popular_problems = popular_problems[popular_problems['rating_count'] >= min_ratings_threshold]\n",
        "\n",
        "\n",
        "#   # Calculate the average rating for each problem\n",
        "#   average_ratings = popular_problems.groupby('problem_id')['rating'].mean().reset_index()\n",
        "#   # Get the top 10 rated problems\n",
        "#   top_10_problems = list(average_ratings.sort_values('rating', ascending=False).head(10).problem_id.values)\n",
        "#   return top_10_problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDY4JvP-o3xZ",
        "outputId": "ba672c7a-66eb-4a65-e5f2-c7a4cd10af7b"
      },
      "outputs": [],
      "source": [
        "# top_10_problems = get_popular_problems(interactions)\n",
        "# [problem_title_dict[problem] for problem in top_10_problems]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9edNBt-ijnA"
      },
      "source": [
        "## 3.5 Recommendations Result Comparison\n",
        "Like the evaluation function we will iterate our validation dataset and store recommendation results in lists to compare them with normalized discounted gain(NDCG) metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7XtOtvbCamoA"
      },
      "outputs": [],
      "source": [
        "# # problem id decoder\n",
        "# problem_vocab_itos = problem_vocab.get_itos()\n",
        "\n",
        "# # A placeholders to store results of recommendations\n",
        "# transformer_reco_results = list()\n",
        "# popular_reco_results = list()\n",
        "\n",
        "# # Get top 10 problems\n",
        "# k = 10\n",
        "# # Iterate over the validation data\n",
        "# for i, (problem_data, user_data) in enumerate(val_iter):\n",
        "#     # Feed the input and get the outputs\n",
        "#     problem_data, user_data = problem_data.to(device), user_data.to(device)\n",
        "#     user_data = user_data.reshape(-1, 1)\n",
        "#     inputs, targets = problem_data[:, :-1], problem_data[:, 1:]\n",
        "#     output = model(inputs, user_data)\n",
        "#     output_flat = output.reshape(-1, ntokens)\n",
        "#     targets_flat = targets.reshape(-1)\n",
        "\n",
        "#     # Reshape the output_flat to get top predictions\n",
        "#     outputs = output_flat.reshape(output_flat.shape[0] // inputs.shape[1],\n",
        "#                                   inputs.shape[1],\n",
        "#                                   output_flat.shape[1])[: , -1, :]\n",
        "#     # k + len(inputs) = 13 problems obtained\n",
        "#     # In order to prevent to recommend already watched problems\n",
        "#     values, indices = outputs.topk(k + inputs.shape[1], dim=-1)\n",
        "\n",
        "#     for sub_sequence, sub_indice_org in zip(problem_data, indices):\n",
        "#         sub_indice_org = sub_indice_org.cpu().detach().numpy()\n",
        "#         sub_sequence = sub_sequence.cpu().detach().numpy()\n",
        "\n",
        "#         # Generate mask array to eliminate already watched problems\n",
        "#         mask = np.isin(sub_indice_org, sub_sequence[:-1], invert=True)\n",
        "\n",
        "#         # After masking get top k problems\n",
        "#         sub_indice = sub_indice_org[mask][:k]\n",
        "\n",
        "#         # Generate results array\n",
        "#         transformer_reco_result = np.isin(sub_indice, sub_sequence[-1]).astype(int)\n",
        "\n",
        "#         # Decode problem to search in popular problems\n",
        "#         target_problem_decoded = problem_vocab_itos[sub_sequence[-1]]\n",
        "#         popular_reco_result = np.isin(top_10_problems, target_problem_decoded).astype(int)\n",
        "\n",
        "#         transformer_reco_results.append(transformer_reco_result)\n",
        "#         popular_reco_results.append(popular_reco_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWid7M9fbx7g"
      },
      "source": [
        "After generating result for each recommendation now time to compare baseline method vs transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DYVH2JmpHrW",
        "outputId": "f250e84f-d158-40ea-ff45-0fbf6ee848e8"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import ndcg_score\n",
        "\n",
        "# # Since we have already sorted our recommendations\n",
        "# # An array that represent our recommendation scores is used.\n",
        "# representative_array = [[i for i in range(k, 0, -1)]] * len(transformer_reco_results)\n",
        "\n",
        "# for k in [3, 5, 10]:\n",
        "#   transformer_result = ndcg_score(transformer_reco_results,\n",
        "#                                   representative_array, k=k)\n",
        "#   popular_result = ndcg_score(popular_reco_results,\n",
        "#                               representative_array, k=k)\n",
        "\n",
        "#   print(f\"Transformer NDCG result at top {k}: {round(transformer_result, 4)}\")\n",
        "#   print(f\"Popular recommendation NDCG result at top {k}: {round(popular_result, 4)}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rvyhAueb2f9"
      },
      "source": [
        "Here we have seen our model results are approximately 10 times better than popular problem recommendation at NDCG metric. A function to generate recommendation for single data is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XAyDjSxV_pZa"
      },
      "outputs": [],
      "source": [
        "def generate_recommendation(user_id, problem_sequence, k=10):\n",
        "    model.eval()\n",
        "    input_sequence = problem_sequence[:-1]\n",
        "    # Tokenize and numerically encode the user id and problem sequence\n",
        "    user_tensor = torch.tensor(user_vocab_stoi[user_id])\n",
        "    problem_tensor = torch.tensor([[problem_vocab_stoi[problem_id]] for problem_id in input_sequence])\n",
        "    # Shape: [1, 1]\n",
        "    user_tensor = user_tensor.unsqueeze(0).to(device)\n",
        "    user_tensor = user_tensor.view(user_tensor.shape[0], 1)\n",
        "\n",
        "    # Shape: [1, seq_length]\n",
        "    problem_tensor = problem_tensor.unsqueeze(0).to(device)[0]\n",
        "    problem_tensor = problem_tensor.view(1, problem_tensor.shape[0])\n",
        "\n",
        "    # Pass the tensors through the model\n",
        "    with torch.no_grad():\n",
        "        predictions = model(problem_tensor, user_tensor)\n",
        "\n",
        "    # The output is a probability distribution over the next problem.\n",
        "    # Topk to get most probable problems\n",
        "    values, indices = predictions.topk(k + len(input_sequence), dim=-1)\n",
        "    # Eliminate already watched problems\n",
        "    indices = [indice for indice in indices[-1, :][0] if indice not in problem_tensor][:k]\n",
        "    predicted_problems = [problem_vocab.get_itos()[problem] for problem in indices]\n",
        "    return predicted_problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1105"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_data_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcuSBAP5fGXb",
        "outputId": "65913364-6919-4cc7-ea9c-c6150884db89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Sequence:\n",
            "-problem_792:E\n",
            "-problem_997:A\n",
            "-problem_588:E\n",
            "-problem_371:D\n",
            "-problem_1542:A\n",
            "-problem_1542:B\n",
            "-problem_579:A\n",
            "Recomendations:\n",
            "-problem_660:D\n",
            "-problem_1451:E1\n",
            "-problem_1909:F1\n",
            "-problem_1228:A\n",
            "-problem_808:D\n",
            "-problem_1013:A\n",
            "-problem_1557:A\n",
            "-problem_484:E\n",
            "-problem_1559:A\n",
            "-problem_25:C\n"
          ]
        }
      ],
      "source": [
        "row_iter = test_data_raw[1019]\n",
        "print(\"Input Sequence:\")\n",
        "print(\"-\" + \"\\n-\".join([ea_problem for ea_problem in row_iter[1][:-1]]))\n",
        "recos = '\\n-'.join(generate_recommendation(row_iter[0],row_iter[1]))\n",
        "\n",
        "print(f\"Recomendations:\\n-{recos}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEdOvaUV4CYQ",
        "outputId": "790082f3-8c68-4336-e587-924701234eb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['user_wxhtzdy',\n",
              "       list(['problem_792:E', 'problem_997:A', 'problem_588:E', 'problem_371:D', 'problem_1542:A', 'problem_1542:B', 'problem_579:A', 'problem_520:B'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "row_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ABofSpZn_9WW"
      },
      "outputs": [],
      "source": [
        "row_iter[0] = '<unk>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7nU2pHxACf1",
        "outputId": "b1f6b087-78d1-4edc-9f11-40cdc227804d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['<unk>',\n",
              "       list(['problem_792:E', 'problem_997:A', 'problem_588:E', 'problem_371:D', 'problem_1542:A', 'problem_1542:B', 'problem_579:A', 'problem_520:B'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "row_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qdiI6gv-Aaip"
      },
      "outputs": [],
      "source": [
        "# user_vocab_stoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMr4txnX3u33",
        "outputId": "c4b5ed46-b90a-4b39-8024-c07f8d82bd39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['problem_1679:C',\n",
              " 'problem_1909:F1',\n",
              " 'problem_1520:F1',\n",
              " 'problem_1593:C',\n",
              " 'problem_1791:G1',\n",
              " 'problem_1244:C',\n",
              " 'problem_1702:G2',\n",
              " 'problem_1279:C',\n",
              " 'problem_1270:E',\n",
              " 'problem_1467:D']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_recommendation(row_iter[0],row_iter[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO1cop5EcU52"
      },
      "source": [
        "# Conclusion\n",
        "In this blog post, we have made an attempt to use the Transformer model, known for its effectiveness in NLP, to create a personalized problem recommendation system. We've gone through from data preprocessing to prediction step using the problemLens dataset. While this is a starting point and there's much more to learn, it hopefully sheds some light on how Transformer models can be used in different contexts, such as recommendation systems.\n",
        "\n",
        "# References\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "https://keras.io/examples/structured_data/problemlens_recommendations_transformers/\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
